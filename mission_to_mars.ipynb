{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from splinter import Browser\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for application\n",
    "def init_browser():\n",
    "    # this code requires chromedriver to be present in the same directory\n",
    "    # as the notebook or python script file\n",
    "    #executable_path = {\"executable_path\": \"./chromedriver.exe\"}\n",
    "    # , **executable_path\n",
    "    return Browser(\"chrome\", headless=False)\n",
    "\n",
    "def scrape_news():\n",
    "    '''scrape_news is a function that uses splinter to access a\n",
    "    NASA url for mars news, it then uses BS4 to scrape the html\n",
    "    and parses it to find the most recent article title and teaser\n",
    "    text. The relevant strings are further extracted from the HTML\n",
    "    and these variables, title and text, are returned as a tuple'''\n",
    "\n",
    "    # Initialize browser\n",
    "    browser = init_browser()    \n",
    "\n",
    "    # Set URL to NASA's Mars News Page\n",
    "    # use splinter browser to navigate headless\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "    time.sleep(3)\n",
    "    # Scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Use soup.find to find instances of div's containing\n",
    "    # the top story's (first result) title and teaser text\n",
    "    title = (soup.find('div',class_='content_title')).text\n",
    "    text = (soup.find('div',class_='article_teaser_body')).text\n",
    "    \n",
    "    # Return results\n",
    "    return title,text\n",
    "\n",
    "def scrape_photo():\n",
    "    '''scrape_photo is a function that utilizes splinter to access the page\n",
    "    https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars and click on the'''\n",
    "\n",
    "    # Initialize browser\n",
    "    browser = init_browser()\n",
    "\n",
    "    # Use the browser to retrieve html\n",
    "    url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    # Use BS4 to extract the featured image URL from the Mars page\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    full_image = soup.find_all(\"li\",class_=\"slide\")\n",
    "    featured_image_url = full_image[0].find('a').get(\"data-fancybox-href\")\n",
    "    # Concat relative URL into absolute URL\n",
    "    featured_image_url = \"https://www.jpl.nasa.gov\"+featured_image_url\n",
    "\n",
    "    # Return URL\n",
    "    return featured_image_url\n",
    "\n",
    "def scrape_weather():\n",
    "    '''scrape_weather is a function that utilizes splinter to acess the curiosity\n",
    "    rover climate twitter page that tweets a once per day weather summary. This function\n",
    "    will return the latest weather tweet (whatever it is!) as a string mars_weather'''\n",
    "    # Initialize browser\n",
    "    browser = init_browser()\n",
    "    # Use the browser to retrieve html\n",
    "    url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    # Use BS4 to extract the latest weather tweet\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    tweets = soup.find_all(\"div\",class_=\"tweet\")\n",
    "    for tweet in tweets:\n",
    "        userid = tweet.find('a').get(\"data-user-id\")\n",
    "        if userid == \"786939553\":\n",
    "            mars_weather = tweet.find(\"p\",class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")\n",
    "            mars_weather = mars_weather.text\n",
    "            _first3 = mars_weather[:3]\n",
    "            if _first3 != 'Sol':\n",
    "                next\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            next\n",
    "    return mars_weather\n",
    "\n",
    "def scrape_fact_table():\n",
    "    '''scrape_fact_table is a very simple python script with no inputs that uses pandas\n",
    "    built in HTML table reader to import from http://space-facts.com/mars/ a table of data\n",
    "    about the red planet. It will then reform the table into HTML code and return it for\n",
    "    deployment to a website'''\n",
    "    list_of_df = pd.read_html(io='http://space-facts.com/mars/',flavor=\"bs4\",header=1,attrs={'id':'tablepress-mars'})\n",
    "    df = list_of_df[0]\n",
    "    html_table = df.to_html()\n",
    "    return html_table\n",
    "\n",
    "def scrape_photos():\n",
    "    '''scrape_photos is an inputless function that navigates using Splinter to the USGS website\n",
    "    of Martian hemispheres. It will use partial link text matches to click to each hemispheres page\n",
    "    record the name of the hemisphere and a link to a full-size image of the hemisphere. It stores these\n",
    "    values in a list of dictionaries with keys: title and img_url and returns that list'''\n",
    "    hemisphere_image_urls = []\n",
    "    # Initialize browser\n",
    "    browser = init_browser()\n",
    "    # Use the browser to navigate the page\n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(url)\n",
    "    # Use click_link_by_partial_text to target the first hemisphere\n",
    "    browser.click_link_by_partial_text('Cerberus Hemisphere')\n",
    "    # Retrieve page HTML\n",
    "    html = browser.html\n",
    "    # Parse HTML with BS4\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    # Extract hemisphere title and full size image URL\n",
    "    title = soup.find('h2',class_=\"title\").text\n",
    "    img_url = soup.find('a',target=\"_blank\").get('href')\n",
    "    # Append dictionary into list\n",
    "    hemisphere_image_urls.append({'title':title,'img_url':img_url})\n",
    "    # Use the back button to navigate back to the main page\n",
    "    browser.click_link_by_partial_text('Back')\n",
    "    # Rinse and repeat for the next 3 hemispheres\n",
    "    browser.click_link_by_partial_text('Schiaparelli Hemisphere')\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    title = soup.find('h2',class_=\"title\").text\n",
    "    img_url = soup.find('a',target=\"_blank\").get('href')\n",
    "    hemisphere_image_urls.append({'title':title,'img_url':img_url})\n",
    "    browser.click_link_by_partial_text('Back')\n",
    "    browser.click_link_by_partial_text('Syrtis Major Hemisphere')\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    title = soup.find('h2',class_=\"title\").text\n",
    "    img_url = soup.find('a',target=\"_blank\").get('href')\n",
    "    hemisphere_image_urls.append({'title':title,'img_url':img_url})\n",
    "    browser.click_link_by_partial_text('Back')\n",
    "    browser.click_link_by_partial_text('Valles Marineris Hemisphere')\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    title = soup.find('h2',class_=\"title\").text\n",
    "    img_url = soup.find('a',target=\"_blank\").get('href')\n",
    "    hemisphere_image_urls.append({'title':title,'img_url':img_url})\n",
    "    return hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'Storm Chasers' on Mars Searching for Dusty Secrets\",\n",
       " \"Scientists with NASA's Mars orbiters have been waiting years for an event like the current Mars global dust storm.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
